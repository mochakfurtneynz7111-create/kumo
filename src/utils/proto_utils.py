"""
All the functions related to clustering and slide embedding construction
"""

import pdb
import os
from utils.file_utils import save_pkl, load_pkl
import numpy as np
import time
from sklearn.cluster import KMeans
from tqdm import tqdm
import torch


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def cluster_leiden(data_loader, feature_dim=1024, n_proto_patches=50000, 
                   resolution=1.0, n_neighbors=15, use_cuda=False):
    """
    Leidenèšç±»è‡ªåŠ¨ç¡®å®šåŽŸåž‹æ•°é‡ + ä¿å­˜ç©ºé—´ä¿¡æ¯
    """
    import scanpy as sc
    import anndata
    
    n_patches = 0
    n_total = n_proto_patches
    
    try:
        n_patches_per_batch = (n_total + len(data_loader) - 1) // len(data_loader)
    except:
        n_patches_per_batch = 1000
    
    print(f"[Leiden] Sampling maximum of {n_total} patches: {n_patches_per_batch} each from {len(data_loader)}")
    
    patches = torch.Tensor(n_total, feature_dim)
    
    # ðŸ”¥ æ–°å¢žï¼šå­˜å‚¨æ¯ä¸ªpatchçš„ç©ºé—´åæ ‡
    patch_coords = torch.Tensor(n_total, 2)  # (x, y)
    
    # === é‡‡æ ·patcheså’Œåæ ‡ ===
    for batch in tqdm(data_loader):
        if n_patches >= n_total:
            continue
        
        data = batch['img']
        coords = batch.get('coords', None)  # å°è¯•èŽ·å–åæ ‡
        
        with torch.no_grad():
            data_reshaped = data.reshape(-1, data.shape[-1])
            np.random.shuffle(data_reshaped)
            out = data_reshaped[:n_patches_per_batch]
        
        size = out.size(0)
        if n_patches + size > n_total:
            size = n_total - n_patches
            out = out[:size]
        
        patches[n_patches: n_patches + size] = out
        
        # ðŸ”¥ æ–°å¢žï¼šä¿å­˜å¯¹åº”çš„åæ ‡
        if coords is not None:
            coords_reshaped = coords.reshape(-1, 2)
            # ä½¿ç”¨ç›¸åŒçš„shuffleå’Œslice
            sampled_coords = coords_reshaped[:n_patches_per_batch][:size]
            patch_coords[n_patches: n_patches + size] = sampled_coords
        else:
            # å¦‚æžœæ²¡æœ‰åæ ‡ï¼Œä½¿ç”¨å‡åæ ‡ï¼ˆé¡ºåºç´¢å¼•ï¼‰
            fake_coords = torch.arange(n_patches, n_patches + size).unsqueeze(1).repeat(1, 2).float()
            patch_coords[n_patches: n_patches + size] = fake_coords
        
        n_patches += size
    
    print(f"\n[Leiden] Total of {n_patches} patches aggregated")
    
    s = time.time()
    
    # === Leidenèšç±» ===
    print(f"\n[Leiden] Running Leiden clustering with resolution={resolution}, n_neighbors={n_neighbors}")
    
    adata = anndata.AnnData(X=patches[:n_patches].cpu().numpy())
    
    print("[Leiden] Computing PCA...")
    sc.tl.pca(adata, svd_solver='arpack', n_comps=min(50, feature_dim-1))
    
    print("[Leiden] Computing neighbors...")
    sc.pp.neighbors(adata, n_neighbors=n_neighbors, n_pcs=50, 
                   method='umap', metric='euclidean')
    
    print("[Leiden] Running Leiden algorithm...")
    sc.tl.leiden(adata, resolution=resolution, key_added='leiden')
    
    leiden_labels = adata.obs['leiden'].astype(int).values
    n_proto = len(np.unique(leiden_labels))
    
    print(f"[Leiden] âœ“ Automatically determined {n_proto} prototypes!")
    
    # === è®¡ç®—åŽŸåž‹ä¸­å¿ƒï¼ˆç‰¹å¾å’Œç©ºé—´ï¼‰ ===
    centroids = []
    proto_spatial_centers = []
    proto_spatial_spreads = []
    proto_to_patches = {}
    
    for c in range(n_proto):
        mask = leiden_labels == c
        
        # ç‰¹å¾ä¸­å¿ƒ
        cluster_patches = patches[:n_patches][mask]
        centroid = cluster_patches.mean(dim=0)
        centroids.append(centroid)
        
        # ðŸ”¥ æ–°å¢žï¼šç©ºé—´ä¸­å¿ƒ
        cluster_coords = patch_coords[:n_patches][mask]
        spatial_center = cluster_coords.mean(dim=0)  # (2,)
        proto_spatial_centers.append(spatial_center)
        
        # ðŸ”¥ æ–°å¢žï¼šç©ºé—´åˆ†æ•£åº¦
        spatial_spread = cluster_coords.std(dim=0).mean().item()
        proto_spatial_spreads.append(spatial_spread)
        
        # è®°å½•åŽŸåž‹åŒ…å«çš„patchesç´¢å¼•
        proto_to_patches[c] = torch.where(torch.from_numpy(mask))[0].numpy()
    
    centroids_matrix = torch.stack(centroids)  # (n_proto, feature_dim)
    proto_spatial_centers = torch.stack(proto_spatial_centers)  # (n_proto, 2)
    proto_spatial_spreads = torch.tensor(proto_spatial_spreads)  # (n_proto,)
    
    # === æž„å»ºç‰¹å¾é‚»å±…å›¾ ===
    print("[Leiden] Computing prototype feature graph...")
    proto_distances = torch.cdist(centroids_matrix, centroids_matrix)
    
    k_neighbors = min(15, n_proto - 1)
    _, topk_indices = proto_distances.topk(k_neighbors + 1, dim=-1, largest=False)
    
    feature_adjacency = torch.zeros(n_proto, n_proto)
    for i in range(n_proto):
        neighbors = topk_indices[i, 1:]  # è·³è¿‡è‡ªå·±
        feature_adjacency[i, neighbors] = 1
        feature_adjacency[neighbors, i] = 1  # å¯¹ç§°åŒ–
    
    # ðŸ”¥ æ–°å¢žï¼šæž„å»ºç©ºé—´é‚»å±…å›¾
    print("[Leiden] Computing prototype spatial graph...")
    spatial_distances = torch.cdist(proto_spatial_centers, proto_spatial_centers)
    
    k_spatial = min(10, n_proto - 1)
    _, spatial_topk = spatial_distances.topk(k_spatial + 1, dim=-1, largest=False)
    
    spatial_adjacency = torch.zeros(n_proto, n_proto)
    for i in range(n_proto):
        neighbors = spatial_topk[i, 1:]
        spatial_adjacency[i, neighbors] = 1
        spatial_adjacency[neighbors, i] = 1
    
    # === ä¿å­˜æƒé‡ ===
    weight = centroids_matrix.unsqueeze(0).numpy()  # [1, C, feature_dim]
    
    e = time.time()
    print(f"[Leiden] Clustering took {e-s:.2f} seconds!")
    print(f"[Leiden] Cluster sizes: {[(leiden_labels==c).sum() for c in range(n_proto)]}")
    print(f"[Leiden] Feature graph edges: {feature_adjacency.sum().item():.0f}")
    print(f"[Leiden] Spatial graph edges: {spatial_adjacency.sum().item():.0f}")
    print(f"[Leiden] Spatial spread: mean={proto_spatial_spreads.mean():.2f}, std={proto_spatial_spreads.std():.2f}")
    
    # ðŸ”¥ æ–°å¢žï¼šè¿”å›žå®Œæ•´ä¿¡æ¯å­—å…¸
    extra_info = {
        'leiden_labels': leiden_labels,
        'proto_to_patches': proto_to_patches,
        'feature_adjacency': feature_adjacency.numpy(),
        'spatial_centers': proto_spatial_centers.numpy(),
        'spatial_spreads': proto_spatial_spreads.numpy(),
        'spatial_adjacency': spatial_adjacency.numpy(),
        'leiden_resolution': resolution,
        'leiden_neighbors': n_neighbors,
        'patch_coords': patch_coords[:n_patches].numpy()  # å¯é€‰ï¼šä¿å­˜åŽŸå§‹åæ ‡
    }
    
    return n_patches, weight, n_proto, extra_info  # è¿”å›ž4ä¸ªå€¼

def cluster_leiden_HPL(data_loader, feature_dim=1024, n_proto_patches=50000, 
                   resolution=1.0, n_neighbors=15, use_cuda=False):
    """
    Leidenèšç±»è‡ªåŠ¨ç¡®å®šåŽŸåž‹æ•°é‡
    
    å‚æ•°è¯´æ˜Ž:
        resolution: Leidenåˆ†è¾¨çŽ‡,æŽ§åˆ¶èšç±»ç²’åº¦
                   - è¶Šå¤§ â†’ èšç±»æ•°è¶Šå¤š (å¦‚2.0å¯èƒ½äº§ç”Ÿ30+ä¸ªèšç±»)
                   - è¶Šå° â†’ èšç±»æ•°è¶Šå°‘ (å¦‚0.5å¯èƒ½äº§ç”Ÿ8-12ä¸ªèšç±»)
                   - æŽ¨èèŒƒå›´: 0.8-1.5
        n_neighbors: é‚»å±…æ•°,ç”¨äºŽæž„å»ºKNNå›¾
    
    è¿”å›ž:
        n_patches: å®žé™…é‡‡æ ·çš„patchæ•°é‡
        weight: åŽŸåž‹ä¸­å¿ƒ [1, C, feature_dim]
        n_proto: è‡ªåŠ¨ç¡®å®šçš„åŽŸåž‹æ•°é‡
    """
    import scanpy as sc
    import anndata
    import pandas as pd
    
    n_patches = 0
    n_total = n_proto_patches  # ä¸å†åŸºäºŽå›ºå®šn_proto
    
    # é‡‡æ ·patches
    try:
        n_patches_per_batch = (n_total + len(data_loader) - 1) // len(data_loader)
    except:
        n_patches_per_batch = 1000
    
    print(f"[Leiden] Sampling maximum of {n_total} patches: {n_patches_per_batch} each from {len(data_loader)}")
    
    patches = torch.Tensor(n_total, feature_dim)
    
    for batch in tqdm(data_loader):
        if n_patches >= n_total:
            continue
        
        data = batch['img']
        with torch.no_grad():
            data_reshaped = data.reshape(-1, data.shape[-1])
            np.random.shuffle(data_reshaped)
            out = data_reshaped[:n_patches_per_batch]
        
        size = out.size(0)
        if n_patches + size > n_total:
            size = n_total - n_patches
            out = out[:size]
        patches[n_patches: n_patches + size] = out
        n_patches += size
    
    print(f"\n[Leiden] Total of {n_patches} patches aggregated")
    
    s = time.time()
    
    # === Leidenèšç±»æ ¸å¿ƒä»£ç  ===
    print(f"\n[Leiden] Running Leiden clustering with resolution={resolution}, n_neighbors={n_neighbors}")
    
    # 1. åˆ›å»ºAnnDataå¯¹è±¡
    adata = anndata.AnnData(X=patches[:n_patches].cpu().numpy())
    
    # 2. æž„å»ºé‚»å±…å›¾(ä½¿ç”¨PCAé™ç»´ä»¥åŠ é€Ÿ)
    print("[Leiden] Computing PCA...")
    sc.tl.pca(adata, svd_solver='arpack', n_comps=min(50, feature_dim-1))
    
    print("[Leiden] Computing neighbors...")
    sc.pp.neighbors(adata, n_neighbors=n_neighbors, n_pcs=50, 
                   method='umap', metric='euclidean')
    
    # 3. Leidenèšç±»
    print("[Leiden] Running Leiden algorithm...")
    sc.tl.leiden(adata, resolution=resolution, key_added='leiden')
    
    # 4. æå–èšç±»æ ‡ç­¾
    leiden_labels = adata.obs['leiden'].astype(int).values
    n_proto = len(np.unique(leiden_labels))  # è‡ªé€‚åº”ç¡®å®šçš„èšç±»æ•°!
    
    print(f"[Leiden] âœ“ Automatically determined {n_proto} prototypes!")
    
    # 5. è®¡ç®—æ¯ä¸ªèšç±»çš„ä¸­å¿ƒä½œä¸ºåŽŸåž‹
    centroids = []
    for c in range(n_proto):
        mask = leiden_labels == c
        cluster_patches = patches[:n_patches][mask]
        centroid = cluster_patches.mean(dim=0)
        centroids.append(centroid)
    
    weight = torch.stack(centroids).unsqueeze(0).numpy()  # [1, C, feature_dim]
    
    e = time.time()
    print(f"[Leiden] Clustering took {e-s:.2f} seconds!")
    print(f"[Leiden] Cluster sizes: {[(leiden_labels==c).sum() for c in range(n_proto)]}")
    
    return n_patches, weight, n_proto  # æ³¨æ„è¿”å›ž3ä¸ªå€¼!


def cluster(data_loader, n_proto, n_iter, n_init=5, feature_dim=1024, 
            n_proto_patches=50000, mode='kmeans', use_cuda=False,
            leiden_resolution=1.0, leiden_neighbors=15):  # æ–°å¢žå‚æ•°
    """
    K-Meansæˆ–Leiden clustering on embedding space
    
    mode: 'kmeans', 'faiss', 'leiden'
    """
    
    # === æ–°å¢ž: Leidenæ¨¡å¼ ===
    if mode == 'leiden':
        return cluster_leiden(
            data_loader, 
            feature_dim=feature_dim,
            n_proto_patches=n_proto_patches,
            resolution=leiden_resolution,
            n_neighbors=leiden_neighbors,
            use_cuda=use_cuda
        )
    # === åŽŸæœ‰K-meansä»£ç ä¿æŒä¸å˜ ===
    
    n_patches = 0
    n_total = n_proto * n_proto_patches

    # Sample equal number of patch features from each WSI
    try:
        n_patches_per_batch = (n_total + len(data_loader) - 1) // len(data_loader)
    except:
        n_patches_per_batch = 1000

    print(f"Sampling maximum of {n_proto * n_proto_patches} patches: {n_patches_per_batch} each from {len(data_loader)}")

    patches = torch.Tensor(n_total, feature_dim)

    for batch in tqdm(data_loader):
        if n_patches >= n_total:
            continue

        data = batch['img'] # (n_batch, n_instances, instance_dim)

        with torch.no_grad():
            data_reshaped = data.reshape(-1, data.shape[-1])
            np.random.shuffle(data_reshaped)
            out = data_reshaped[:n_patches_per_batch]  # Remove batch dim

        size = out.size(0)
        if n_patches + size > n_total:
            size = n_total - n_patches
            out = out[:size]
        patches[n_patches: n_patches + size] = out
        n_patches += size

    print(f"\nTotal of {n_patches} patches aggregated")

    s = time.time()
    if mode == 'kmeans':
        print("\nUsing Kmeans for clustering...")
        print(f"\n\tNum of clusters {n_proto}, num of iter {n_iter}")
        kmeans = KMeans(n_clusters=n_proto, max_iter=n_iter)
        kmeans.fit(patches[:n_patches].cpu())
        weight = kmeans.cluster_centers_[np.newaxis, ...]

    elif mode == 'faiss':
        assert use_cuda, f"FAISS requires access to GPU. Please enable use_cuda"
        try:
            import faiss
        except ImportError:
            print("FAISS not installed. Please use KMeans option!")
            raise
        
        numOfGPUs = torch.cuda.device_count()
        print(f"\nUsing Faiss Kmeans for clustering with {numOfGPUs} GPUs...")
        print(f"\tNum of clusters {n_proto}, num of iter {n_iter}")

        kmeans = faiss.Kmeans(patches.shape[1], 
                              n_proto, 
                              niter=n_iter, 
                              nredo=n_init,
                              verbose=True, 
                              max_points_per_centroid=n_proto_patches,
                              gpu=numOfGPUs)
        
        kmeans.train(patches.numpy())
        weight = kmeans.centroids[np.newaxis, ...]

    else:
        raise NotImplementedError(f"Clustering not implemented for {mode}!")

    e = time.time()
    print(f"\nClustering took {e-s} seconds!")

    return n_patches, weight

def check_prototypes(n_proto, embed_dim, load_proto, proto_path):
    """
    Check validity of the prototypes
    """
    if load_proto:
        assert os.path.exists(proto_path), "{} does not exist!".format(proto_path)
        if proto_path.endswith('pkl'):
            prototypes = load_pkl(proto_path)['prototypes'].squeeze()
        elif proto_path.endswith('npy'):
            prototypes = np.load(proto_path)


        assert (n_proto == prototypes.shape[0]) and (embed_dim == prototypes.shape[1]),\
            "Prototype dimensions do not match! Params: ({}, {}) Suplied: ({}, {})".format(n_proto,
                                                                                           embed_dim,
                                                                                           prototypes.shape[0],
                                                                                           prototypes.shape[1])

